{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiN_sPfz94Z2"
   },
   "source": [
    "# Deep Learning for Media \n",
    "#### MPATE-GE 2039 - DM-GY 9103 \n",
    "\n",
    "---\n",
    "\n",
    "## Homework 2 \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Complete parts 1 through 5, filling in code in the `utils.py` and `models.py` files where indicated **# YOUR CODE HERE** or responses in `this notebook` where marked with **# YOUR RESPONSE HERE**.\n",
    "2. **Document** your code. Add comments explaining what the different parts of your code are doing. \n",
    "3. If you copy code from external resources (which is fine), include references as comments.\n",
    "4. When finished, commit and push this completed notebook file along with the `utils.py` file, `models.py` file, and `any other files indicated in the instructions below` to your GitHub repository corresponding to this homework.\n",
    "5. **IMPORTANT:** do not modify any of the provided code.\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "To ensure that your code passes the autograder, we encourage you to use the following package versions when completing the assignment:\n",
    "\n",
    "```\n",
    "python                       3.9.0 or higher\n",
    "opencv-python                4.9.0 or higher\n",
    "keras                        2.15.0\n",
    "tensorflow                   2.15.0\n",
    "```\n",
    "\n",
    "**Grading:** \n",
    "\n",
    "- This homework is worth 10 points.\n",
    "- Each function you code is worth 1 point, for a total of 6 points. \n",
    "- Each answer in part 4 is worth 1 point, for a total of 4 points.\n",
    "- Points will be automatically assigned when passing tests, and manually assigned when it comes to your written responses.\n",
    "\n",
    "**Academic integrity:**\n",
    "\n",
    "Remember that this homework should be authored by you only. It's ok to discuss with classmates but you have to submit your own original solution.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "Notebook based on the companion materials of: \n",
    "\n",
    "<blockquote>\n",
    "\"Deep Learning with Python\", Second Edition by  F. Chollet, 2021.\n",
    "</blockquote>\n",
    "\n",
    "You are encouraged to follow the code in `Chapter 8` from the book as help for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43hwvPI-9wMu"
   },
   "source": [
    "## Small Data, Big Challenge: Improving Models with Data Augmentation and Transfer Learning\n",
    "\n",
    "---\n",
    "\n",
    "In this exercise, you will improve the performance of a convolutional model trained with a small dataset of dog and cat images. Many real-world applications involve small datasets. Therefore, it's essential for you to learn how to handle small datasets effectively.\n",
    "\n",
    "You will start by training a simple convolutional model with the limited amount of data provided. You will then observe the model's overfitting behavior and identify the need for regularization techniques such as data augmentation. You will then augment the dataset and re-train the model.\n",
    "\n",
    "Even with data augmentation, the model may not generalize well to new data due to the limited amount of training data variability. Therefore, you will use transfer learning, where you will leverage pre-trained models to improve the performance of your model. Happy coding!\n",
    "\n",
    "**BEFORE YOU CONTINUE:** Make sure you're using GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KET4KUDEbkY"
   },
   "outputs": [],
   "source": [
    "# This notebook is for visualization and written answers only\n",
    "# Your code should go in utils.py and models.py\n",
    "import utils as u # to use the functions from utils, do u.function()\n",
    "import models as m # to use the functions from models, do m.function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdwScnt2EkoD"
   },
   "outputs": [],
   "source": [
    "# Fix the random seed for reproducibility\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HVZ7egUErNy"
   },
   "source": [
    "## Part 1 - Organizing the data\n",
    "\n",
    "Load the Dogs vs. Cats Small dataset, which consists of images of dogs and cats of different sizes. Check out the sizes of the different subsets (train, validation, test), and explore the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2L7928SGSFB"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbl4YWJlGUrZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_home = '/content/drive/My Drive/dl4m_datasets/dogs-vs-cats_small/' \n",
    "if not os.path.exists(data_home):\n",
    "  print('You should download the dataset first, unzip all its folders and '\n",
    "  'locate it in the data_home folder.\\nFollow the instructions in the practice '\n",
    "  'of our class #6.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jsLxMNpG7pU"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "\n",
    "target_shape = (180, 180)\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = u.create_dataset(os.path.join(data_home, 'train'), \n",
    "                                 batch_size, target_shape)\n",
    "\n",
    "val_dataset = u.create_dataset(os.path.join(data_home, 'validation'), \n",
    "                               batch_size, target_shape)\n",
    "\n",
    "test_dataset = u.create_dataset(os.path.join(data_home, 'test'), \n",
    "                                batch_size, target_shape, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVywmzGZH72T"
   },
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "# WARNING: loading all the data might take a bit of time\n",
    "u.explore_data(train_dataset, data_home, ['cat', 'dog'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGXuVjB19wM1"
   },
   "source": [
    "## Part 2 - Build, train and evaluate a baseline\n",
    "\n",
    "Build a convolutional baseline model that classifies cats vs. dogs. Note that your model should solve a binary classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXP2KxOZJLwd"
   },
   "outputs": [],
   "source": [
    "# Build baseline\n",
    "input_shape = (180, 180, 3)\n",
    "baseline = m.build_baseline(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNTdYdQw9wM2"
   },
   "outputs": [],
   "source": [
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOyBzUV5LdC0"
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "import keras\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"baseline.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = baseline.fit(\n",
    "              train_dataset, \n",
    "              validation_data=val_dataset,\n",
    "              epochs=30, \n",
    "              batch_size=batch_size,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehDFO8SNMLYj"
   },
   "outputs": [],
   "source": [
    "u.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VpMsh7YNK-t"
   },
   "outputs": [],
   "source": [
    "# # Load the best checkpoint of the model\n",
    "test_model = keras.models.load_model(\"baseline.keras\")\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = test_model.evaluate(test_dataset)\n",
    "print('Loss: %.4f, Accuracy: %.2f%%' % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b33g0S329wM5"
   },
   "source": [
    "## Part 3 - Regularize the model\n",
    "\n",
    "Regularize the model by using data augmentation and dropout. There are many different augmentations that you can apply to the model, we will do horizontal flipping (which you will implement) and random rotation (which is already implemented). Besides data augmentation, we will include a dropout layer to further regularize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1Ta1xvc9wM6"
   },
   "outputs": [],
   "source": [
    "# Obtain the augmented datasets\n",
    "\n",
    "train_dataset_aug = u.create_dataset(os.path.join(data_home, 'train'), \n",
    "                                 batch_size, target_shape, augment=True)\n",
    "\n",
    "val_dataset_aug = u.create_dataset(os.path.join(data_home, 'validation'), \n",
    "                                 batch_size, target_shape, augment=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMwu_lft9wM6"
   },
   "source": [
    "**Displaying some randomly augmented training images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vec3oM6NBTLp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for batch, _ in val_dataset_aug.take(1):\n",
    "  for image in batch[:5]:\n",
    "    # print(image.numpy().max())\n",
    "    for i in range(9):\n",
    "        aug_img = image.numpy()\n",
    "        if np.random.rand() > 0.5:\n",
    "          aug_img = u.flip_image(image.numpy())\n",
    "        angle = np.random.uniform(-40, 40)\n",
    "        aug_img = u.rotate_image(aug_img, angle)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(aug_img)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iign-iU79wM6"
   },
   "source": [
    "**Defining a new convnet that includes image augmentation and dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wmTZJY39wM6"
   },
   "outputs": [],
   "source": [
    "aug_model = m.build_reg_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCsfrrGPYpEN"
   },
   "outputs": [],
   "source": [
    "aug_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hcm-GsvNLUu"
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "import keras\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"aug_model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = aug_model.fit(\n",
    "              train_dataset_aug, \n",
    "              validation_data=val_dataset_aug,\n",
    "              epochs=30, \n",
    "              batch_size=batch_size,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MkBexlMY-ao"
   },
   "outputs": [],
   "source": [
    "u.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lpggL8KY-ao"
   },
   "outputs": [],
   "source": [
    "# Load the best checkpoint of the model\n",
    "aug_model = keras.models.load_model(\"aug_model.keras\")\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = aug_model.evaluate(test_dataset)\n",
    "print('Loss: %.4f, Accuracy: %.2f%%' % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc8BhrmZx6dH"
   },
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "import numpy as np\n",
    "np.save('results/predictions_da.npy', aug_model.predict(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4VMKGz19wM7"
   },
   "source": [
    "## Part 4 - Leveraging a pretrained model\n",
    "\n",
    "We are going to use the VGG16 convolutional model trained with Imagenet as our pre-trained model. We will extract features from it, and use them to train a small classifier build from dense layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a50KIs8h9wM8"
   },
   "source": [
    "**Instantiating the VGG16 convolutional base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mei97vde9wM8"
   },
   "outputs": [],
   "source": [
    "conv_base = m.load_conv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJOkpoMF9wM8"
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84Ra8tzb9wM9"
   },
   "source": [
    "**Extracting the VGG16 features and corresponding labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5GWpqqRF9_H"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "\n",
    "train_dataset = u.create_dataset(os.path.join(data_home, 'train'), \n",
    "                                 batch_size, target_shape, normalize=False)\n",
    "\n",
    "val_dataset = u.create_dataset(os.path.join(data_home, 'validation'), \n",
    "                               batch_size, target_shape, normalize=False)\n",
    "\n",
    "test_dataset = u.create_dataset(os.path.join(data_home, 'test'), \n",
    "                                batch_size, target_shape, \n",
    "                                shuffle=False, normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qr9dPZwu9wM9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_features, train_labels =  u.get_features_and_labels(train_dataset, conv_base)\n",
    "val_features, val_labels =  u.get_features_and_labels(val_dataset, conv_base)\n",
    "test_features, test_labels =  u.get_features_and_labels(test_dataset, conv_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyI6tb9e9wM9"
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr1rqYDp9wM9"
   },
   "source": [
    "**Defining and training the densely connected classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VycNa6Qh9wM9"
   },
   "outputs": [],
   "source": [
    "dense_model = m.build_dense_model(input_shape=train_features.shape[1:])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "history = dense_model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmSiVC7Y9wM9"
   },
   "source": [
    "**Plotting the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaZwDX7yFrOs"
   },
   "outputs": [],
   "source": [
    "u.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvekT3NAvU8F"
   },
   "outputs": [],
   "source": [
    "# Load the best checkpoint of the model\n",
    "transf_model = keras.models.load_model(\"feature_extraction.keras\")\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = transf_model.evaluate(test_features, test_labels)\n",
    "print('Loss: %.4f, Accuracy: %.2f%%' % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9My9O3D5xty0"
   },
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "np.save('results/predictions_tl.npy', transf_model.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o40KHG8Sy5e"
   },
   "source": [
    "## Part 5 - Discuss the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is overfitting? How does it affect the performance of a model trained on a small dataset? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# YOUR RESPONSE HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o40KHG8Sy5e"
   },
   "source": [
    "2. What are the benefits of data augmentation in improving the performance of a model trained on a small dataset? Provide an example of a data augmentation technique that could be used in this exercise other than flipping and rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# YOUR RESPONSE HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o40KHG8Sy5e"
   },
   "source": [
    "3. What is transfer learning? How can it be used to improve the performance of a model trained on a small dataset? Provide an example of a pre-trained model that could be used in this exercise beyond VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# YOUR RESPONSE HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o40KHG8Sy5e"
   },
   "source": [
    "4. What problems are you interested in where transfer learning might be useful and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# YOUR RESPONSE HERE`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1Jdl_TT9VL0-3gP61rqLsfn5ebyX_GBv5",
     "timestamp": 1678669813896
    },
    {
     "file_id": "1xWJfpPu6wipSOLgw9j2xxvUJPEUNZteL",
     "timestamp": 1678294034104
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
